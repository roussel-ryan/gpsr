import math
from abc import abstractmethod, ABC
from typing import Any

import torch
from torch import Size, Tensor
from torch.nn import Module
from torch.distributions import MultivariateNormal, Distribution

from cheetah.particles import ParticleBeam
from cheetah.utils.bmadx import bmad_to_cheetah_coords

import zuko


class BeamGenerator(torch.nn.Module, ABC):
    @abstractmethod
    def forward(self) -> ParticleBeam:
        pass


class NNTransform(torch.nn.Module):
    def __init__(
        self,
        n_hidden: int,
        width: int,
        dropout: float = 0.0,
        activation: Module = torch.nn.Tanh(),
        output_scale: float = 1e-2,
        phase_space_dim: int = 6,
    ):
        """
        Nonparametric transformation - NN
        """
        super(NNTransform, self).__init__()

        layer_sequence = [torch.nn.Linear(phase_space_dim, width), activation]

        for i in range(n_hidden):
            layer_sequence.append(torch.nn.Linear(width, width))
            layer_sequence.append(torch.nn.Dropout(dropout))
            layer_sequence.append(activation)

        layer_sequence.append(torch.nn.Linear(width, phase_space_dim))

        self.stack = torch.nn.Sequential(*layer_sequence)
        self.register_buffer("output_scale", torch.tensor(output_scale))

    def forward(self, X: Tensor) -> Tensor:
        return self.stack(X) * self.output_scale


class NNParticleBeamGenerator(BeamGenerator):
    def __init__(
        self,
        n_particles: int,
        energy: float,
        base_dist: Distribution = None,
        transformer: NNTransform = None,
        n_dim: int = 6,
    ):
        super(NNParticleBeamGenerator, self).__init__()
        self.base_dist = base_dist or MultivariateNormal(
            torch.zeros(n_dim), torch.eye(n_dim)
        )
        self.transformer = transformer or NNTransform(
            2, 20, output_scale=1e-2, phase_space_dim=n_dim
        )
        self.register_buffer("beam_energy", torch.tensor(energy))
        self.register_buffer("particle_charges", torch.tensor(1.0))
        self.register_buffer("survival_probabilities", torch.ones(n_particles))

        self.set_base_particles(n_particles)

    def set_base_particles(self, n_particles: int):
        self.register_buffer(
            "base_particles", self.base_dist.sample(Size([n_particles]))
        )

    def forward(self) -> ParticleBeam:
        transformed_beam = self.transformer(self.base_particles)

        # create near zero coordinates into which we deposit the transformed beam
        # Note: these need to be near zero to maintain finite emittances
        bmad_coords = torch.randn(len(transformed_beam), 6).to(transformed_beam) * 1e-7
        bmad_coords[:, : transformed_beam.shape[1]] = transformed_beam

        transformed_beam = bmad_to_cheetah_coords(
            bmad_coords, self.beam_energy, torch.tensor(0.511e6)
        )
        return ParticleBeam(
            *transformed_beam,
            particle_charges=self.particle_charges,
            survival_probabilities=self.survival_probabilities,
        )


class GenModel(torch.nn.Module, ABC):
    """Abstract base class for generative models.

    Subclasses implement the method `_sample` to generate samples {z_i} ~ p(z)
    for "normalized" coordinate vector z. Subclasses may also implement `_log_prob`
    to evaluate the logarithm of the probability density p(z). The actual coordinates
    generated by the `sample` method are obtained by a linear transformation x = Lz,
    where L is defined by the Cholesky decomposition of a covariance matrix S = LL^T.
    The probability densities in the two spaces are related by p(x) = p(z) / |det(L)|.

    The covariance matrix S is chosen by the user and defaults to the identity matrix.
    If the normalized distribution p(z) has identity covariance matrix (<zz^T = I),
    then the transformed distribution p(z) will have covariance matrix <xx^T> = S.
    Thus, the covariance matrix can be chosen to avoid learning the scale and linear
    correlations between variables if these are known beforehand.
    """

    def __init__(self, ndim: int, cov_matrix: torch.Tensor = None) -> None:
        super().__init__()

        self.ndim = ndim

        cov_matrix = torch.clone(cov_matrix)
        if cov_matrix is None:
            cov_matrix = torch.eye(self.ndim)
        self.register_buffer("cov_matrix", cov_matrix)

        unnorm_matrix = torch.linalg.cholesky(cov_matrix)
        unnorm_matrix_log_det = torch.log(torch.abs(torch.linalg.det(unnorm_matrix)))
        norm_matrix = torch.linalg.inv(unnorm_matrix)

        self.register_buffer("unnorm_matrix", unnorm_matrix)
        self.register_buffer("unnorm_matrix_log_det", unnorm_matrix_log_det)
        self.register_buffer("norm_matrix", norm_matrix)

    @abstractmethod
    def _sample(self, n: int) -> torch.Tensor:
        """Generate samples {z_i} in the normalized space."""
        pass

    @abstractmethod
    def _log_prob(self, z: torch.Tensor) -> torch.Tensor:
        """Evaluate log probabilities {log(p(z_i))} in the normalized space."""
        pass

    @abstractmethod
    def _sample_and_log_prob(self, n: int) -> tuple[torch.Tensor, torch.Tensor]:
        """Generate samples {z_i} and log probabilities {log(p(z_i))} in the normalized space."""
        pass

    def sample(self, n: int) -> torch.Tensor:
        """Generate samples {x_i}."""
        z = self._sample(n)
        x = torch.matmul(z, self.unnorm_matrix.T)
        return x

    def log_prob(self, x: torch.Tensor) -> torch.Tensor:
        """Evaluate log probabilities {log(p(x_i))}."""
        z = torch.matmul(x, self.norm_matrix.T)
        log_prob = self._log_prob(z)
        log_prob = log_prob - self.unnorm_matrix_log_det
        return log_prob

    def sample_and_log_prob(self, n: int) -> tuple[torch.Tensor, torch.Tensor]:
        """Generate samples {x_i} and log probabilities {log(p(x_i))}."""
        z, log_prob = self._sample_and_log_prob(n)
        x = torch.matmul(z, self.unnorm_matrix.T)
        log_prob = log_prob - self.unnorm_matrix_log_det
        return (x, log_prob)


class NSFDist(GenModel):
    """Implements a normalizing flow using rational-quadratic splines.

    Note that the RQS transformation is only defined in the range [-5, 5].

    This class uses the Zuko library: https://github.com/probabilists/zuko/blob/master/zuko/flows/autoregressive.py
    """

    def __init__(
        self,
        transforms: int = 3,
        hidden_layers: int = 3,
        hidden_units: int = 64,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)

        self._flow = zuko.flows.NSF(
            features=self.ndim,
            transforms=transforms,
            hidden_features=(hidden_layers * [hidden_units]),
        )
        # Reverse for faster sampling
        self._flow = zuko.flows.Flow(self._flow.transform.inv, self._flow.base)

    def _sample(self, n: int) -> torch.Tensor:
        return self._flow().rsample((n,))

    def _log_prob(self, z: torch.Tensor) -> torch.Tensor:
        return self._flow().log_prob(z)

    def _sample_and_log_prob(self, n: int) -> tuple[torch.Tensor, torch.Tensor]:
        return self._flow().rsample_and_log_prob((n,))

    def sample_base(self, n: int) -> torch.Tensor:
        return self._flow.base().sample((n,))

    def to(self, device):
        self.flow = self.flow.to(device)
        self.norm_matrix = self.norm_matrix.to(device)
        self.unnorm_matrix = self.unnorm_matrix.to(device)
        self.unnorm_matrix_log_det = self.unnorm_matrix_log_det.to(device)
        return self


class EntropyBeamGenerator(BeamGenerator):
    """Generates beam and entropy in forward pass."""

    def __init__(
        self,
        gen_model: GenModel,
        prior: Any,
        n_particles: int,
        energy: float,
        mass: float = 0.511e06,
        particle_charges: float = 1.0,
    ) -> None:
        """Constructor.

        gen_model: Generative model
        prior: Prior distribution over the phase space coordiantes. Must implement
               `prior.log_prob(x: torch.Tensor) -> torch.Tensor`, where `x` is
               a batch of particle coordinates.
        n_particles: Number of macro-particles in the beam
        energy: Reference particle energy [eV].
        mass: Reference particle mass [eV/c^2]. Defaults to electron mass.
        particle_charges: Macro-particle charges [C].
        """
        super(EntropyBeamGenerator, self).__init__()

        self.n_dim = 6
        self.n_particles = n_particles

        self.gen_model = gen_model
        self.prior = prior

        self.register_buffer("energy", torch.tensor(energy))
        self.register_buffer("mass", torch.tensor(mass))
        self.register_buffer("particle_charges", torch.tensor(particle_charges))

    def set_base_particles(self, n_particles: int) -> None:
        self.n_particles = n_particles

    def forward(self) -> tuple[ParticleBeam, torch.Tensor]:
        x, log_p = self.gen_model.sample_and_log_prob(self.n_particles)

        log_q = 0.0
        if self.prior is not None:
            log_q = self.prior.log_prob(x)

        entropy = -torch.mean(log_p - log_q)

        particles, ref_energy = bmad_to_cheetah_coords(x, self.energy, self.mass)
        particles[:, 4] *= -1.0  # [TO DO] why is sign wrong?
        beam = ParticleBeam(
            particles, energy=ref_energy, particle_charges=self.particle_charges
        )
        return (beam, entropy)


class Prior(torch.nn.Module, ABC):
    @abstractmethod
    def log_prob(self, x: torch.Tensor) -> torch.Tensor:
        pass

    def sample(self, n: int) -> torch.Tensor:
        pass


class GaussianPrior(Prior):
    def __init__(self, loc: torch.Tensor, cov: torch.Tensor) -> None:
        super().__init__()

        self.ndim = len(loc)

        self.register_buffer("loc", loc)
        self.register_buffer("cov", cov)
        self.register_buffer("cov_inv", torch.linalg.inv(cov))
        self.register_buffer("log_cov_det", torch.log(torch.linalg.det(cov)))
        self.register_buffer("L", torch.linalg.cholesky(cov))

    def log_prob(self, x: torch.Tensor) -> torch.Tensor:
        _x = x - self.loc
        _log_prob = 0.0
        _log_prob -= 0.5 * torch.sum(_x * torch.matmul(_x, self.cov_inv.T), axis=1)
        _log_prob -= 0.5 * (self.ndim * math.log(2.0 * math.pi) + self.log_cov_det)
        return _log_prob

    def sample(self, n: int) -> torch.Tensor:
        x = torch.randn((n, self.ndim))
        x = torch.matmul(x, self.L.T)
        return x
